1. Introduction and Word Vectors

**Human language is a system** specifically constructed **to convey meaning**, and is not produced by a physical manifestation of any kind. There are some exceptions, when we use words and letters for expressive signaling. On top of this, the symbols of language can be encoded in several modalities: voice, gesture, writing, etc that are transmitted via *continuous* signals to the **brain, which itself appears to encode things in a continuous manner.**

The first and arguably most important common denominator across all NLP tasks is how we represent words as input to any of our models. To perform well on most NLP tasks we first need to have some notion of **similarity and difference between words**. 


## 2.Word Vectors

The most intuitive reason for word vectors is that perhaps there actually exists some *N*-dimensional space that is sufficient to encode all semantics of our language.

* **One-hot** vectors: Represent every word as an  vector with all  0s and one 1 at the index o that word in the sorted english language (dimension problem).
* **SVD Based Methods**: After accumulating word co-occurrence counts in some form of a matrix , we perform **Singular Value Decomposition** on  to get a  decomposition, where we use the rows of  as the word embeddings for all words in out dictionary. The condition of  can be mainly grouped as follows:

  1. **Word-Document Matrix**: That is to say,  denotes as the number of word  appearing in document (the co-occurence of words in the same document).
  2. **Window based Co-occurrence Matrix**: , with the scale , stores co-occurrences of words thereby becoming an affinity matrix.  stands for the number of times each word appears inside a window of a particular size around the word of interest (count the number of times each word appears inside a  
      window of a particular size around the word of interest).

  Using Word-Word Co-occurence matrix:

  * Generate $|V|\times|V|$ co-occurrence matrix, $X$.
  * Apply **SVD** on $X$ to get $X = USV^T$, to compute our final word vector for each word $w_j$, we can simply sum $w_j = u_j + v_j$(alternatively, we could concatenate $u_j, \; v_j$)
  * select the first $k$ columns of $U$ to get a $k$-dimensional word vectors (Here, $\frac{\sum_{i=1}^k \sigma_i}{\sum_{i=1}^|V| \sigma_i}$ indicates the amount of variance captured by the first $k$ dimensions)

  **The flaws of SVD based method**: It does not scale well for big matrices and it is hard to incorporate new words or documents. Computational cost for a $m\times n$ matrix is $O(mn^2)$  

  3. **Positive pointwise mutual information**:
  4. **Neural network based models**: 

      * Skip-gram/continuous bag of words:
      * Global vectors for word representation(GloVe)

## 3. Word2vec

The meaning of a word can be represented or estimated by its contextual words. **Word2vec** is a software package that actually includes:

* **2 algorithms**: *continuous bag-of-words*(CBOW) and *skip-gram*. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram predicts the distribution (probability) of context words from a center word.
* **2 training methods**: *negative sampling* and *hierarchical softmax*. Negative sampling defines an objective by ***sampling negative examples***, while hierarchical softmax defines an objective using an efficient ***tree structure*** to compute probabilities for all the vocabulary.

**Assumption**: Bigram model  $P(w_1, w_2, \cdots, w_n) = \prod_{i=2}^n P(w_i|w_{i-1})$

Continuous bag of words(**CBOW**):<br />

* **Problem description**:  given the context, which  consist of a string of words, predict the center word.  In probability language, we want to estimate the distribution .
* **Structure**: $n$ is an arbitrary size which defines the size of embedding space. some notions: (1) $w_i$: Word $i$ from vocabulary ; (2) $V \in R^{n\times |V|}$: input word matrix; (3) $v_i$: $i$-th column of , the input vector representation of  word $w_i$; (4) $U \in R^{|V| \times n }$: output word matrix; (5) $u_i$: $i$-th row of $U$, the output vector representation of word s.
* **Algorithm steps**:

  **Input**: a sentence whose length is set as fixed window/context size $2m+1$.  
  **output**: a center word  
  **hypothesis**: $u_c = \arg \max_{u_i \in V} p(u_i \;|\;\text{context})$  

  1. We generate one hot word vectors for the input context of size $m$: $(x_{c-m}, \cdots, x_{c-1}, x_{c+1}, \cdots, x_{c+m})\in R^{|V|}$
  2. We get our embedded word vectors for the context: $v_i = V \cdot x_(i) \in R^n, i \in \{c-m, \cdots, c -1, c+1, \cdots, c+m\}$
  3. Average these vectors to get context vector $\hat v =\frac{\sum_i v_i}{2m} \in R^n , i \in \{c-m, \cdots, c -1, c+1, \cdots, c+m\}$
  4. Generate a score vector $z = U \hat  v\in R^{|V|}$.  As the dot product of similar vectors is higher, it will push similar words close to each other in order to achieve a high score.
  5. Turn the scors into probabilities $\hat y = \text {softmax}(z) \in R^{|V|}$ .
  6. We desire out probabilities generated, $\hat y \in R^{|V|}$, to match the true probabilities, $y \in R^{|V|}$, which also happens to be the one hot vector of the actual word.

  **objective function**: $y$ is a one-hot vector, so we can get cross entropy  loss as$H(\hat y, y) = - \sum_{j = 1}^{|V|} y_j\log(\hat y_j) = -y_i \log(\hat y_i)$. 

  $$
  minimize\; J = -\log P(w_c|w_{c-m},\cdots,w_{c-1}, w_{c+1}, w_{c+m})\\
  =-\log P(u_c|\hat v)\\
  =-\log \frac{\exp (u_c^T\hat v)}{\sum_{j=1}^{|V|} \exp(u_c^T\hat v)}\\
  =-u_c^T\hat v + \log \sum_{j=1}^{|V|} \exp(u_c^T\hat v)
  $$

**Skip-Gram Model**: Given center word, to predict context words. the output of softmax is the sum of one hot vectors of context.

* **Problem description**:  given the context, which  consist of a string of words, predict the center word.  In probability language, we want to estimate the distribution .
* **Structure**: $n$ is an arbitrary size which defines the size of embedding space. some notions: (1) $w_i$: Word $i$ from vocabulary ; (2) $V \in R^{n\times |V|}$: input word matrix; (3) $v_i$: $i$-th column of , the input vector representation of  word $w_i$; (4) $U \in R^{|V| \times n }$: output word matrix; (5) $u_i$: $i$-th row of $U$, the output vector representation of word s.
* **Algorithm steps**:

  **Input**: a sentence whose length is set as fixed window/context size $2m+1$.  
  **output**: a context word  
  **hypothesis**: $u_c = \arg \max_{u_i \in V} p(\text{context} \;|\; u_c)$  

  1. We generate one hot input vectors $x \in R^{|V|}$ of the center word.
  2. We get our embedded word vectors for the center word: $v_c = V \cdot x \in R^n$
  3. Generate a score vector $z = U v_c\in R^{|V|}$.
  4. Turn the scors into probabilities $\hat y = \text {softmax}(z) \in R^{|V|}$ . Note that $\hat y_{c-m}, \hat y_{c-1}, \hat y_{c+1}, \hat y_{c+m}$ are the probabilities of observing each context word.
  5. We desire our probabilities vector generated, $\hat y_i \in R^{|V|}, i \in \{c-m, \cdots, c-1, c+1, \cdots, c+m\}$, to match the true probabilities, which is  $y_i \in R^{|V|}, i \in \{c-m, \cdots, c-1, c+1, \cdots, c+m\}$, the one hot vectors of the actual output.

  **objective function**: (in some ways, it can be taken as the sum of $2m$ loss items in CBOW)

  $$
  minimize\; J = -\log P(w_{c-m},\cdots,w_{c-1}, w_{c+1}, w_{c+m}|w_c)\\
  =-\log \prod_{j=0, j \neq m}^{2m}  P(w_{c-m+j}|w_c)\\
  =-\log \prod_{j=0, j \neq m}^{2m}  P(u_{c-m+j}|v_c)\\
  =-\log \frac{\exp (u_{c-m+j}^Tv_c)}{\sum_{j=1}^{|V|} \exp(u_j^T v_c)}\\
  =-\sum_{j=0, j \neq m}^{2m} u_c^T\hat v + 2m \log \sum_{j=1}^{|V|} \exp(u_c^T\hat v)
  $$

**Negative Sampling**: Note that the summation over $|V|$ is computationally huge! A simple idea is we could instead just approximate it. For every training step, instead of looping over the entire vocabulary, we can just sample several negative examples. We "sample" from a noise distribution $P_n(w)$ whose probabilities match the ordering of the frequency of the vocabulary. To augement our formulation of the problem to incorporate  negative sampling, all we need to do is update the: (1) objective function; (2) gradients; (3) update rules.

* **objective function**: Consider a pair  $(w,c)$ of word and context. The probability that  $(w,c)$ came from the  corpus data is denoted by $P(D=1|w,c)$ and that came from negative sampling is denoted by $P(D=0|w,c)$. Let's model  with the sigmoid function:

$$
P(D=1|w,c,\theta) = \sigma (v_c^T v_w) = \frac{1}{1+\exp(-v_c^Tv_w)}
$$

Now, we build a new objective function that tries to maximize the probability of a word and context being in the corpus data if it indeed is, and maximize the probability of a word and context not being in the corpus data if it indeed is not( Here we take $\theta$ to be the parameters of the model, and in our case it is $V \;\text{and}\; U$).

$$
\theta = \arg\max_\theta \prod_{(w,c) \in D} P(D=1|w,c,\theta)\prod_{(w,c)\in \~D} P(D=0|w,c,\theta)\\
=\arg\max_\theta \prod_{(w,c) \in D} P(D=1|w,c,\theta)\prod_{(w,c)\in \~D} (1 - P(D=1|w,c,\theta))\\
=\arg\max_\theta \sum_{(w,c)\in D} \log P(D=1|w,c,\theta) + \sum_{(w,c)\in \~D} \log(1 - P(D=1|w,c,\theta))\\
=\arg\max_\theta \sum_{(w,c)\in D} \log \frac{1}{1+ exp(-u_w^Tv_c)} + \sum_{(w,c)\in \~D} \log \frac{1}{1+ exp(u_w^Tv_c)} \\
= \arg \min_\theta -\sum_{(w,c)\in D} \log \frac{1}{1+ exp(-u_w^Tv_c)} - \sum_{(w,c)\in \~D} \log \frac{1}{1+ exp(u_w^Tv_c)}
$$

    For skip-gram, our new objective function for observing the context word $c-m+j$ given the center word $c$ would be $-\log \sigma(u_{c-m+j}^T\cdot v_c) - \sum_{k=1}^{K} \log (-\~u_k^T\cdot v_c)$  
    For CBOW, our new objective function for observing the center word $u_c$ given the context vector $\hat v =\frac{\sum_i v_i}{2m} \in R^n , i \in \{c-m, \cdots, c -1, c+1, \cdots, c+m\}$ would be $-\log \sigma(u_{c}^T\cdot \hat v) - \sum_{k=1}^{K} \log (-u_c^T\cdot \hat v)$.  
    In the above formulation, $\{\~u_k|k=1,\cdots,K\}$ are sampled from $P_n(w)$. What seems to work best is the Unigram Model to the power of $3/4$ (words with low frequency/probability are $3\times$ more, like $0.01^{3/4} = 0.032$; words with high frequency/probability only went up marginally, like $0.9^{3/4}=0.92$).

* **gradients**:
* **update rules**:

Negative sampling works better for freuent words and lower dimensional vectors.

**Hierarchical softmax** uses a binary tree to represent all words in the vocabulary. Each leaf of the tree is a word, an there is a unique path from root to leaf. In this model, there is no output representation for words, and the probability of a word $w$  given a vector $w_i$, $P(w|w_i)$, is equal to the probability of a random walk starting  in the root and ending in the leaf node corresponding to $w$. 


Hierachical softmax tends to be better for infrequent words.


id: d135e3714a8c4438aba1f5274dc7f744
parent_id: 51a1435e58cb4207b3693dfbea6589d8
created_time: 2023-03-26T03:26:51.966Z
updated_time: 2023-03-31T12:46:42.245Z
is_conflict: 0
latitude: 0.00000000
longitude: 0.00000000
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 1680266802239
user_created_time: 2023-03-26T03:26:51.966Z
user_updated_time: 2023-03-31T12:46:42.245Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
type_: 1