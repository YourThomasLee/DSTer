2020 A survey on bayesian deep learning

Hao Wang,  Dit Yan Yeung

## 1. 背景

> A comprehensive artificial intelligence system needs to not only perceive the environment with different ‘senses, but also infer the world’s conditional (or even causal) relations and corresponding uncertainty. In this general framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in turn, the feedback from the inference process by probabilistic graphical models with their Bayesian nature is able to enhance the perception of text or images
>
> 深度学习在感知能力上为复杂的任务提供了一个良好的工具, 能够在许多复杂任务上提供性能上的提升, 但在高层次的推理和灵活性上, 基于贝叶斯的概率图模型仍然非常出色. 将二者结合起来的称为贝叶斯深度学习, 贝叶斯深度学习具有如下优势[1]:
>
> 1. 良好的准确率 to achieve high accuracy in recommender systems
> 2. 能够处理以图片等的非线性控制系统 dealing with the control of non-linear dynamical systems with raw images as input
> 3. 提供了一个统合概率图模型和深度学习的框架 provides a principled way of unifying deep learning and PGM, another benefit comes from the implicit regularization built in BDL.
> 4. 提供了参数的不确定性视角，包括感知的不确定性，任务相关推理的不确定性和感知与任务信息传递的不确定性 provides a principled bayesian approach of handling parameter uncertainty: 1) uncertainty on the neural network parameters; 2) uncertainty of the task-specific parameters; 3) uncertainty of exchanging information between the perception component and the task-specific component.[1]
>
> 贝叶斯深度学习的缺点是:
>
> 1. 算法复杂度高 it is non-trivial to design an efficient bayesian formulation of neural networks with reasonable time complexity
> 2. 必须仔细设计信息交换模块 to ensure efficient and effective information exchange between the perception component and the task-specific component[1]
> 3. 依赖于高维数据和有效的不确定性模型 information exchange between the perception task and the inference task, conditional dependencies on high-dimensional data, and effective modeling of uncertainty
>
> 参考的github库: https://github.com/js05212/BayesianDeepLearning-Survey
>

说明: 这篇论文提供了一个贝叶斯深度学习框架, 但是在hinge part上并没有一个比较清晰的归纳, 都是借助例子来说明. 在表示学习和任务相关的部分, 归纳了一下经典的技术, 可以一看.

## 2. 贝叶斯深度学习基础

深度学习通产认为是多于两层的神经网络.  在此文中, 可以先了解一些简单的多层感知机(Multilayer perceptrons, MLP)来去建立直接的印象.

### 2.1 多层感知机

实际上, 多层感知机是一系列的非线性变换. 假设我们需要训练多层感知机去实现一个M维向量映射到D维向量的回归任务. 假设原始的输入$X_0$(0层感知机 [ 0-th layer of perceptron ] 的输入), 用$X_{0,j*}$表示输入矩阵的$j$行. 想要拟合的向量为$Y$, 用$Y_{j*}$表示矩阵的$j$行. 训练$L$层的感知机的学习任务可以表示为以下优化问题

$$
\min_{W_l, b_l} ||X_L - Y|| + \lambda \sum_l ||W_l||^2_F \\ 
subject \;to\; X_l=\sigma(X_{l-1}W_l + b_l), l=1,2,\cdots,L-1\\
X_L=X_{L-1}W_L+b_L
$$

此处$sigma(\cdot)$是一个位级别映射函数$\sigma(x)=\frac{1}{1+exp(-x)}$, $||\cdot||_F$表示一个Frobenius norm(一个范数). 训练一般基于链式求导得到梯度使用随机梯度下降算法实现.

### 2.2 自动编码器(Auto-encoders, AE)

自动编码器是通过全连接网络对输入进行编码并将其重构表示为更为紧凑/低维的输出(An autoencoder (AE) is a feedforward neural network to encode the input into a more compact representation and reconstruct the input with the learned representation. ) 在最简单的形式中, 一个自动编码器就是一个多层感知机加一个瓶颈（bottleneck）层.

在这里我们介绍以这个多层降噪自动编码器(denosing AE), 也称作堆叠降噪自动编码器(stacked denoising autoencoders, SDAE). SDAE是一个在给定含有噪声的输入前提下预测无噪声的输入神经网络, 其结构可见下面的图片

![SDAE.png](:/4c7082582c124dae81affa8bbe781d6b "SDAE")


SDAE主要做以下优化:

$$
\min_{W_l, b_l} ||X_c-X_L||_F^2 + \lambda \sum_l ||W_l||^2_F\\
subject \;to\; X_l=\sigma(X_{l-1}W_l+b_l),\; l=1,\cdots,L-1\\
X_L=X_{L-1}W_L+b_L
$$

这里$X_c, X_0$分别是不含噪声和含噪声的输入. 中间层$X_2$将用来作为原始数据的表示输入到网络中.  自动编码器的目标就是寻找复杂输入背后的简洁表示。

### 2.3 卷积神经网络(Convolutional neural networks, CNN)

CNN可以被认为是MLP的一种变体. 与AE不同的是, 这个网络一开始就是设计用来做降维. 卷积神经网络包含两个关键的概念: 卷积和池化.


![CNN.png](:/41e95100a1644c649648bbf80bb5f6ea "CNN")

特征图是线性变换和激活函数的输入, 它可以是原始图像或者上一层网络的输出. 对于给定输入$X$, 权重$W^k$和偏置$b^k$, $k$层网络的特征图$H^k$可以按以下方式获得

$$
H^k_{ij}=\tanh ((W^k\cdot X)_{ij} + b^k)
$$

最大池化(Max-pooling): 对一个小区域进行下采样, 生成更小规格的特征图.

### 2.4 循环神经网络(Recurrent neural network)

循环神经网络（vanilla recurrent neural network）: 给定前一时刻的状态$h_{t-1}$和当前时刻的输入$x_t$，那么

$$
a_t = Wh_{t-1} + Yx_t+b\\
h_t=\tanh(a_t)\\
o_t=Vh_t+c
$$

其中，$h_t$为当前状态的估计，$v_t$为当前时刻的输出，$W,Y,V$为权重向量，$b,c$为对应的偏置。

![image.png](:/bab89e507325433aa8b20fdc5aa7fe33)

门机制循环神经网络（gated recurrent neural network）：RNN可能出现的问题是梯度爆炸和消失问题，考虑到这个情况，门机制循环神经网络被提出，其中比较经典的工作是LSTM（long short-term memory model）

$$
x_t=W_w e_t\\
s_t=h_{t-1}^f\odot s_{t-1} + h_{t-1}^i\odot\sigma(Yx_{t-1}+wh_{t-1}+b)\\
h_t^f=\sigma(Y^fx_t+W^fh_t+b^f) \; [forget gate]\\
h_t^i=\sigma(Y^ix_t+W^ih_t+b^i)\; [input gate]\\
h_t=\tanh(s_t)\odot h^o_{t-1},\;\; h_t^o=\sigma(Y^ox_t+W^oh_t+b^o) [output]
$$

其中，$\odot$ 表示两个向量间的位乘法

## 3 概率图模型(probabilistic graphical models, PGM)

概率图模型使用图来表示随机变量和他们之间的关系, 概率图模型主要有两种, 一种是有向概率图(贝叶斯网络, bayesian networks), 另外一种是无向概率图(马尔科夫随机场, markcov random fields). 

![image.png](:/3c3bcb740cf2470cb8efc830a5feb2f5)

一个典型的概率图模型是潜狄利克雷分布(latent dirichlet allocation, LDA), 这个通常会用来分析字, 话题和文档的生成. 通常一个概率图模型会伴随着一个对应的随机变量生成过程. 对于LDA, 其过程如下

![LDA.png](:/663aa4c3261e428e894ae90e96021e18 "LDA生成过程")

严格来说，寻找参数的值的过程被称为学习(learning)，寻找隐变量的过程被称为推导(inference)

对于概率图模型, 其学习和推断也有相应的算法和工具. 其中最有效的也许是最大后验估计(maximum a posteriori probability), MAP能够提供一个有效的点估计, 但他不能将贝叶斯理论中的不确定性考虑并发挥作用, 在这方面, 可以考虑使用变分推断(variational inference)和马尔科夫链门特卡罗(Markov chain monte carlo, MCMC)


## 4 贝叶斯深度学习

贝叶斯深度学习的基本框架

* 通常由两个部分组成，感知组件和任务处理组件。通常来说感知组件会被建模成概率公式，设计成链状的变量概率图并使用网络实现。而对于任务处理组件则通常依赖于具体的任务，例如，它可以使一个传统的贝叶斯网络，或者是一个随机过程。

![PGM.png](:/9367f4011d504bc9a860f24509821751 "PGM实例")

* 有三类变量：感知变量(perception variables, $\Omega_p$)，信息交换变量(hinge variables, $\Omega_h$)，任务变量(task variables, $\Omega_t$)

  * 有监督学习 $p(\Omega_p,\Omega_h,\Omega_t) = p(\Omega_p)p(\Omega_h|\Omega_p)p(\Omega_t|\Omega_h)$
  * 无监督学习 $p(\Omega_p,\Omega_h,\Omega_t) = p(\Omega_t)p(\Omega_h|\Omega_t)p(\Omega_p|\Omega_h)$;

  对于$\Omega_h$一般有三种设定，方差为0，方差为一个超参数，方差为一个分布，其灵活性和复杂度依次递增，目前得到最好效果的是超参数设定（说明学习算法有待改进）

* 学习算法：实际的学习算法必须符合 1. 可以在线学习； 2. 线性复杂度。第一条否定了传统的变分推断算法和MCMC算法，通常需要使用它们的在线版本呢。除了MAP推断，大多数随机梯度算法由于不符合bayesian的设定不能够使用


### 4.1 感知组件

最完美的情况一般是感知模块被设计陈一个概率或者贝叶斯网络，为了兼容后续的任务导向组件。此处主要介绍部分典型的感知组件

* 受限波兹曼机（Restricted Boltzman Machine, RBM）是一类特殊的不使用反向传播的贝叶斯网络，变量被设定成为二元的。定义能量为$E(v, h) = -v^TWh-v^tb-h^Ta$, 其中$v$代表可观测的神经元，$h$被定义为隐藏的神经元。 $W,a,b$是可训练的参数。基于能量函数，可以定义以下条件分布

  $$
  p(v|h)=\frac{\exp(-E(v,h)}{\sum_v\exp(-E(v,h))}\\
  p(h|v)=\frac{\exp(-E(v,h)}{\sum_h\exp(-E(v,h))}
  $$

  RBM使用“Contrastive Divergence”进行训练。
* Probabilistic Generalized SDAE
* Variational Autoencoders
* Natural-Parameter Networks


### 4.4 任务导向组件

一般的

* Bayesian Networks
* Bidirectional Inference Networks
* Stochastic Process




## 5. 具体的贝叶斯深度学习模型和应用



### 5.1 推荐系统中的有监督贝叶斯深度学习


### 5.2 话题模型中无监督贝叶斯深度学习


### 5.3 控制论中的贝叶斯深度表示学习


### 5.4 其他应用




the predictive distribution: $p(y|x, D) = \int p(y|x, w) p(w|D) dw$.

Deep learning normally refers to neural networks with more than two layers. deep learning (深度学习) 中常见的一些组件:

* autoencoders(自动编码器):
* convolution neural networks(卷积神经网络):
* recurrent neural networks(循环神经网络):
* transformer:

probabilistic graphical model(概率图模型, PGM)常见的一些组件:

* directed PGM: bayesian networks, LDA
* undirected PGM: Markov random fields

the process of finding the parameters is called learning and the process finding the latent variables given the parameters is called inference[1], 如果包含隐藏节点, 那就是推断, 如果没有, 那就是学习.

* maximum posterior (MAP): probabilistic matrix factorization (PMF)
* variational inference
* markov chain monte carlo

贝叶斯深度学习的基本框架:

1. structure: consists of two parts - the task-specific component and the perception component.
2. elements: three sets of variables, perception  variables, hinge variables, and task variables.
3. I.I.D. Requirement: the connections between hinge variables and the perception component should be i.i.d. for convenience of parallel computation in the perception component.
4. uncertainty of exchanging information between the perception component and the task-specific component, which can be represented by the conditional density $p(V_h|V_p)$ [1]

对应的范例有:

1. stacked denoising autoencoders(SDAE)
2. bayesian SDAE(recommender system)
3. marginalized SDAE(recommender system)
4. relational stacked denoising autoencoder model(RSDAE)
5. deep poisson fator analysis with sigmoid belief networks
6. bayesian deep learning for control. [1]

提供大规模性质的核心技术有:

1. stochastic optimization
2. variance reduction
3. doubly stochastic variational inference
4. scalable MCMC algorithms


对于复杂任务, 有三类不确定性是需要考虑的:

* Uncertainty on the neural network parameters.
* Uncertainty on the task-specific parameters
* Uncertainty of exchanging information between the perception component and the task-specific component.




Reference:  
[1]  Wang H, Yeung D Y. Towards Bayesian deep learning: A framework and some existing methods[J]. IEEE Transactions on Knowledge and Data Engineering, 2016, 28(12): 3395-3408.  
[2] Wilson A G , Izmailov P . Bayesian Deep Learning and a Probabilistic Perspective of Generalization[J]. 2020.  
[3] Wang H, Yeung D Y. A survey on Bayesian deep learning[J]. ACM Computing Surveys (CSUR), 2020, 53(5): 1-37.


id: d69de2c077384157bd11208bf76f4478
parent_id: 5f10f0934cfa48b9abbf98ec93b6762f
created_time: 2023-03-26T03:26:51.966Z
updated_time: 2023-03-31T13:51:08.689Z
is_conflict: 0
latitude: 0.00000000
longitude: 0.00000000
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 1680266802354
user_created_time: 2023-03-26T03:26:51.966Z
user_updated_time: 2023-03-31T12:46:42.376Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
type_: 1