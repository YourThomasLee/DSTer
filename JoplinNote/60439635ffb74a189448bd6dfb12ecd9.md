Notes - deep learning - a bayesian perspective

Bayesian probabilistic perspective provides a number insights into more efficient algorithms for optimization and hyper-parameter tuning.

In comparison to traditional high-dimensional data reduction techniques, such as principal component analysis(PCA), partial least squares (PLS), reduced rank regression (RRR), projection pursuit regres-
sion (PPR) are all shown to be shallow learners. Their deep learning counterparts exploit
multiple deep layers of data reduction which provide predictive performance gains

Stochastic gradient descent (SGD) training optimisation and Dropout (DO) regularization provide
**estimation and variable selection**. Bayesian regularization is central to finding weights and
connections in networks to optimize the predictive bias-variance trade-off

dropout：可以视作多种结构模型的average。

id: 60439635ffb74a189448bd6dfb12ecd9
parent_id: 092da0c34f3d46b58883166b6be22040
created_time: 2023-06-15T09:55:55.658Z
updated_time: 2023-06-16T01:55:30.926Z
is_conflict: 0
latitude: 31.23041600
longitude: 121.47370100
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 0
user_created_time: 2023-06-15T09:55:55.658Z
user_updated_time: 2023-06-16T01:55:30.926Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
type_: 1