Lecture 1. Bayesian framework

## Lecture 1. Bayesian framework

### 课程内容

本节课主要包含以下内容：

1.  Bayes theorem
2.  frequentist vs. Bayesian
3.  generative and discriminative models
4.  learning Bayesian models
5.  advantages of Bayesian ML models

首先第一部分Bayes理论需要理解以下内容：

1.Conditional distribution

$$
Conditional=\frac{Joint}{Marginal}, p(x|y)=\frac{p(x,y)}{p(y)}

$$

2.Product rule: 任何联合分布都可以被表达为一维条件分布的乘积：

$$
p(x,y,z)=p(x|y,z)p(y|z)p(z)
=p(z|x,y)p(x|y)p(y)

$$

理解：任意联合分布都可以从单个随机变量观察拓展到总体观察，但这个定理引入的一个问题是，如何找到一个最好的观察链路

3.Sum rule: 任何边际分布都可以表达为联合分布在无关随机变量上的积分

$$
p(y)=\int p(x,y) dx=\int p(y|x)p(x) dx=E_x p(y|x)

$$

任意的边际分布都是联合分布的抽象，可以认为是从其他随机变量进行观察的期望。具体到事件可以变为全概率公式，全概率公式可以看成是“由原因推结果的公式”

这三个定理是我们能够推导出许多我们想要估计或假设的模型变量的分布。bayes理论可以写为如下形式：

$$
Posterior=\frac{Likelihood \times Prior}{Evidence}

$$

$$
p(x|y)=\frac{p(y|x)p(x)}{p(y)}

$$

bayes公式的意义：在$y$已经发生的条件下，贝叶斯公式可用来寻找导致$y$发生的各种原因$x$的概率

第二部分内容： 贝叶斯学派和频率学派的比较。考虑典型的统计推断问题：

> Given i.i.d. data $X=(x_1,x_2,\cdots,x_n)$ from distribution$p(x|\theta)$ one needs to estimate $\theta$.
> 
> - Maximum likelihood estimation(MLE):
>     
>     $\theta_{ML}=arg \max \prod_{i=1}^{n}p(X|\theta) =arg \max \sum_{i=1}^{n}\log p(x_i|\theta)$
>     
> - Bayesian inference: encode uncertainty about $\theta$ in terms of a distribution $p(\theta)$ and apply Bayesian inference
>     
>     $p(\theta|X)=\frac{\prod_{i=1}^{n}p(x_i|\theta)p(\theta)}{\int \prod_{i=1}^{n}p(x_i|\theta)p(\theta)d\theta}$
>     

如下为在理论框架和算法层面上的具体比较

|     | Frequentist | Bayesian |
| :--- | :---: | :---: |
| Randomness | Objective indefiniteness | Subjective ignorance |
| Variables | Random and deterministic | Everything is random |
| Inference | Maximal likelihood | Bayes theorem |
| Estimates | ML-estimates | Posterior or MAP-estimates |
| Applicability | n >> d | $\forall n$ |

关于两个模型的关系可以理解为：

$$
\lim_{n/d \rightarrow \infty}p(\theta|x_1,\cdots,x_n)=\delta(\theta-\theta_{ML})

$$

$\delta(x)$是物理学中常用的一个函数，它的特点是在除0以外的值全为0，但在整个定义域上的积分为1。它的一个性质是**挑选性**：$\int_{-\infty}^{+\infty}f(x)\delta(x-t_0)dx=f(t_0)$.

$$
\theta_{ML}= arg \max p(X|\theta)=arg \max \prod_{i=1}^{n}p(x_i|\theta)
\\\log \theta_{ML}= arg \max \log \prod_{i=1}^{n} p(X|\theta)=arg \max \sum_{i=1}^{n}\log p(x_i|\theta)

$$

$$
\theta_{MAP}=arg \max p(\theta|X)=arg \max \frac{p(X|\theta)p(\theta)}{p(X)}
\\\log \theta_{MAP}=arg \max \log p(\theta|X)=arg \max \log (\frac{p(X|\theta)p(\theta)}{p(X)})
\\=arg \max (\log p(X|\theta)+\log p(\theta)-\log p(X))
\\=arg \max (\log p(X|\theta)+\log p(\theta))+C
\\When\; p(\theta) \sim Gaussian(\theta,\sigma^{2}) \sim e^{-\frac{\theta^2}{2\sigma^2}},\;\log \theta_{MAP}=arg \max \log p(X|\theta)+\frac{\theta^2}{2\sigma^{2}}

$$

结论为 **Frequentist framework is a limit case of Bayesian one!**

第三部分生成判别分析模型的基础概念，再次回忆下机器学习：

> 1.  机器学习试图找到数据内部隐藏的规律(ML tries to find regularities within the data).
> 2.  Data is a set of objects(users, images, signals, RNAs, Chemical compounds, credit histories, etc. )
> 3.  Each object is described by a set of observed variables $X$ and a set of hidden (latent) variabilities $T$
> 4.  It is assumed that the values of hidden variables are hard to get and we have only limited number of objects with known hidden variables, so-called training set ($X_{tr},T_{tr}$).
> 5.  The goal is to find the of predicting the hidden variables for a new object given the values of observed variables by adjusting the eights $W$ of decision rule.

概率判别模型则可表示为如下：

> 1.  Observed variables are assumed to be known for all objects
> 2.  Models $p(T,W|X)$ thus not modelling the distribution of observed variables
> 3.  usually assumes that prior over W does not depend on $X$:$p(T,W|X)=p(T|X,W)p(W)$
> 4.  Models $p(T,W|X)$ thus not modelling the distribution of observed variables

概率生成模型则可以理解为：

> 1.  Models joint distribution over all variables $p(X,T,W)=p(X,T|W)p(W)$
> 2.  Given trained algorithm we may generate new objects, i.e. pairs ($x,t$)
> 3.  May be quite difficult to train since space of $X$ is usually much more complicated than space of $T$

第四部分Bayes模型：Training bayesian models

- Suppose we are given training data ($X_{tr},T_{tr}$) and a discriminative model $p(T,W|X)$.
    
- At training stage we perform Bayesian inference over $W$:$P(W|X_{tr},T_{tr})=\frac{p(T_{tr}|X_{tr},W)p(W)}{\int p(T_{tr}|X_{tr},W)p(W)dW}$, thus obtaining **ensemble** of algorithms rather than a single one.
    
- At test stage new data $x$ arrives and we need to compute the predictive distribution on its hidden value $t$. To do this we perform ensembling w.r.t. posterior over the weights $W$, $p(t|x,X_{tr},T_{tr})=\int p(t|x,W)p(W|X_{tr},T_{tr})dW$.
    
    ​ Ensembling really helps and outperforms single best algorithm within model.
    
    ​ Posterior $p(W|X_{tr},T_{tr})$ contains **all** information about dependencies between $X$ and $T$ that the model could extract
    
- If new labeled data ($X_{tr}^{'},T_{tr}^{'}$) arrives we may skip the old training data and update our algorithm only on new data using $p(W|X_{tr},T_{tr})$ as a new prior
    

Bayes理论中存在的困难与阻碍，有二：

1.  $P(W|X_{tr},T_{tr})=\frac{p(T_{tr}|X_{tr},W)p(W)}{\int p(T_{tr}|X_{tr},W)p(W)dW}$中的积分项计算, 一般无法解析得到
2.  $p(t|x,X_{tr},T_{tr})=\int p(t|x,W)p(W|X_{tr},T_{tr})dW$中的积分项计算, 一般无法解析得到

考虑到这种困难, 从理论角度来解决这种计算问题--共轭分布(后验分布和先验分布是相同的)

> **Distribution $p(y)$ and $p(x|y)$ are conjugate iff $p(y|x)$ belongs to the same parametric family as p(y), i.e.**
> 
> $$
> p(y) \in A(\alpha), p(x|y) \in B(y), then \  p(y|x) \in \ A(\alpha')
> 
> $$
> 
> **in this case Bayesian inference can be done in closed form**

**例1**(Beta-Bernoulli model). Consider Bayesian model $p(x,\theta)=p(x|\theta)p(\theta),\, x\in \{0,1\} \; and \; \theta \in [0,1]$ of following form

$$
p(x|\theta)=B(x|\theta)=\theta^{x}(1-\theta)^{1-x}
\\p(\theta)=Bet(\theta|a,b)=\frac{1}{R(a,b)}\theta^{a-1}(1-\theta)^{b-1}

$$

where $R(a,b)=\int_0^1 \xi^{a-1}(1-\xi)^{b-1}d\xi$.

根据Bayes框架，首先第一步计算后验概率$p(\theta|X),\; X=(x_1,\cdots,x_n)$. 代入公式：

$$
p(\theta|X)=\frac{p(X|\theta)p(\theta)}{\int p(X|\theta)p(\theta)d\theta}=\frac{1}{Z} \prod_{i=1}^{n} p(x_i|\theta)p(\theta)
\\=\frac{1}{Z}\prod_{i=1}^{n}\theta^{x_i}(1-\theta)^{1-x_i}\frac{1}{R(a,b)}\theta^{a-1}(1-\theta)^{b-1}
\\=\frac{1}{Z'}\theta^{a+\sum_{i=1}^{n}x_i-1}(1-\theta)^{b+n-\sum_{i=1}^{n}x_i-1}
\\=\frac{1}{Z'}\theta^{a'-1}(1-\theta)^{b'-1}=Bet(\theta|a',b')
\\where\;a'=a+\sum_{i=1}^{n}x_i,\;b'=b+n-\sum_{i=1}^{n}x_i

$$

已经发现的共轭分布：

![共轭分布](:/9ec8df61adcf455dbb5ce8377bc4cc37)

简化的概率建模：

1.  Approximate posterior $p(W|X_{tr},T_{tr})$ with a $\delta$ function $\delta(W-W_{MP})$.
    
2.  Corresponds to point estimate of $W$: $W_{MP}=arg \max p(W|X_{tr},T_{tr})=arg \max p(T_{tr}|X_{tr},W)p(W)$
    
3.  Inference is more simple.
    
    $$
    p(T|X,X_{tr},T_{tr})= \int p(T|X,W)p(W|X_{tr},T_{tr})dW \approx p(T|X,W_{MP})
    
    $$
    

最后一部分，Bayes框架相对于其他模型的优势：

> - Regularization(规则化调整)
>     - 使用先验分布$p( \theta )$调整最大似然估计(带权重的最大似然估计)
>     - 避免过拟合；
>     - 自动先验概率分布选择(we can set the best prior automatically by performing Bayesian model selection)
> - Latent variable modeling(lecture 2)
>     - we may build models with latent variables that are unknown at training stage
>     - allows to process missing data
>     - allows to build and train much more complicated mixture models
> - Extend-ability：$p(\theta) \rightarrow p(\theta | X_1) \rightarrow p(\theta|X_1,X_2)\rightarrow \cdots$
> - Scalability(lecture 5)
>     - Bayesian methods is computationally fast now for new mathematical tools proposed like scalable variational approximations and MCMC algorithms(lecture 3,4,14,15)
>     - Now it is applicable to large datasets and high dimensions

### 课后习题

1.  problem description(Insufficient Bayesian Evidence):

> Setting: The Dark Mark stays with 20% probability if the maker is die and stays with 100% probability if the maker is still alive. The Dark Lord survived his attack on Harry Potter with the small chance of 1:100.
> 
> Objective: Given Severus Snapes Dark Mark has not faded, find the probability of the Dark Lord being alive.

Solution: Let $x \in \{0,1\}$ indicate that the Dark Lord is alive, $y \in \{0,1\}$ indicate that Dark Mark is still visible. So the problem is compute $p(x=1|y=1)$.

$$
p(x=1|y=1)=\frac{p(y=1|x=1)p(x=1)}{p(y=1)}=\frac{p(y=1|x=1)p(x=1)}{\sum_j p(y=1|x=j)p(x=j)}
\\=\frac{1\times \frac{1}{101}}{1\times \frac{1}{101}+\frac{1}{5}\times \frac{100}{101}}=\frac{1}{21}

$$

2.  problem description(MLE for multinomial likelihood):

> $D=\{x_1,\cdots,x_N\}$ \- independent dice rolls
> 
> $N_k=\sum_{n=1}^{N} \Pi(x_n=k)$ \- counts
> 
> $p(D|\theta)=\prod_{k=1}^{K}\theta_k^{N_k}$ -multinomial likelihood, $\theta \in S_k$
> 
> Objective: Maximum likelihood estimate for $\theta$, $arg \max_{\theta \in S_K} \log p(D|\theta)$.

Solution: $\theta$ is restricted to simplex. Change parameterization to $\mu_k=\log \theta_k,\; \mu_k \in R$ to the omit inequality restrictions. The Lagrangian has form $L(\mu,\lambda)=\log p(D|\exp(\mu))+\lambda(\sum_{k=1}^{K}\exp(\mu_k)-1)=\sum_{k=1}^{K}(N_k\mu_k-\lambda\exp(\mu_k))-\lambda$. After differentiation we find the singular point $\theta_k=\frac{N_k}{\sum_{l=1}^{K}N_l},\, k=1,\cdots,K$:

$$
0=\frac{\partial L(\mu,\lambda) }{\partial \mu_k}=N_k-\lambda \exp(\mu_k) \Rightarrow \theta_k=\exp(\mu_k)=\frac{N_k}{\lambda}\\
0=\frac{\partial L(\mu,\lambda) }{\partial \lambda}=\sum_{k=1}^{K}\exp(\mu_k)-1\Rightarrow \lambda=\sum_{k=1}^{K}N_k

$$

3.  problem description (Dirichlet-multinomial model):

> Check that the Dirichlet prior $Dir(\theta|\alpha)=\frac{\Pi(\theta \in S_K)}{B(\alpha_1,\cdots,\alpha_K)}\prod_{k=1}^{K}\theta_{k}^{\alpha_{k}-1}$ is the conjugate prior for the multinomial likelihood $p(D|\theta)=\prod_{k=1}^{K}\theta_k^{N_k}$ by computing the posterior $p(\theta|D,\alpha)$.( As $B(\alpha_1,\cdots,\alpha_K)=\int_{S_K}\prod_{k=1}^{K}\theta_{K}^{\alpha_k-1}d\theta$ )
> 
> Compute the posterior predictive $p(x_{N+1}=k|D,\alpha)=\int_{S_K}p(x_{N+1}=k|\theta)p(\theta|D,\alpha)d\theta$.

Solution: Apply the Bayes rule $p(\theta|D,\alpha)=\frac{p(D|\theta)p(\theta|\alpha)}{p(D|\alpha)}$. Denominator (a.k.a. evidence).

$$
p(D|\alpha)=\int_{S_K} p(D|\theta)p(\theta|\alpha) d\theta=\frac{\int_{S_K} \prod_{k=1}^{K}\theta_k^{N_k}\theta_{k}^{\alpha_k-1}d\theta}{B(\alpha_1,\cdots,\alpha_K)}\\=\frac{B(\alpha_1+N_1,\cdots,\alpha_K+N_K)}{B(\alpha_1,\cdots,\alpha_K)}

$$

and use the resulting expression

$$
p(\theta|D,\alpha)=\frac{p(D|\theta)p(\theta|\alpha)}{p(D|\alpha)}=\frac{B(\alpha_1,\cdots,\alpha_K) \Pi(\theta\in S_K)\prod_{k=1}^K\theta_k^{N_k}\theta_{k}^{\alpha_k-1}}{B(\alpha_1+N_1,\cdots,\alpha_K+N_K)B(\alpha_1,\cdots,\alpha_K)}\\
=Dir(\theta|\alpha')

$$

where $\alpha'=(\alpha_1+N_1,\cdots,\alpha_K+N_K)$.

$$
p(x_{N+1}=k|\alpha)=\int_{S_K}p(x_{N+1}=k|\theta)p(\theta|D,\alpha)d\theta=\frac{\int_{S_K}\theta_k\prod_{k=1}^{K}\theta_{k}^{N_k+\alpha_k-1}d\theta}{B(\alpha_1+N_1,\cdots,\alpha_K+N_K)}\\
=\frac{B(\alpha_1+N_1,\cdots,\alpha_k+N_k+1,\cdots,\alpha_K+N_K)}{B(\alpha_1+N_1,\cdots,\alpha_k,\cdots,\alpha_K+N_K)}\\
=\frac{\Gamma(\alpha_1+N_1)\cdots\Gamma(\alpha_k+N_k+1)\cdots\Gamma(\alpha_K+N_K)}{\Gamma(\alpha_1+N_1)\cdots\Gamma(\alpha_k+N_k)\cdots\Gamma(\alpha_K+N_K)}\cdot\frac{\sum_l(\alpha_l+N_l)}{\sum_l((\alpha_l+N_l)+1)}\\
=\frac{\alpha_k+N_k}{\sum_k\alpha_k+N}

$$

4.  problem description (Bayesian decision theory):

> For loss functions, $L_0(\theta,\theta')=1-\delta(\theta-\theta')$, $L_2(\theta,\theta')=(\theta-\theta)^T(\theta-\theta')$ and a posterior distribution $p(\theta|D)$. find point estimates $arg \min_{\theta' \in S_K} E_{p(\theta|D)}L(\theta,\theta')$. Compute these estimates for the Dirichlet-multinomial model $p(D|\theta)Dir(\theta|\alpha)$.

Solution: $L_0$: compute the expectation $E_{p(\theta|D)}L_0(\theta,\theta')=\int_{S_K}p(\theta|D)(1-\delta(\theta-\theta'))d\theta=1-p(\theta'|D)$. Its minimum is achieved at any distribution mode.

$L_2$: The only stationary point is expectation $\theta'=E_{p(\theta|D)}\theta$

$$
\nabla_{\theta'}E_{p(\theta|D)}L_2(\theta,\theta')=\nabla_{\theta'}\int_{S_K}p(\theta|D)(\theta-\theta')^T(\theta-\theta')d\theta\\
=2\int_{S_K}p(\theta|D)(\theta-\theta')d\theta=\theta'-E_{p(\theta|D)}\theta=0

$$

$L_0$: The optimization problem is the same as in problem 2: $C\cdot\prod_{k=1}^{K}\theta_k^{\alpha_k+N_k-1}\rightarrow\max_{\theta\in S_K}$. The optimal value $\theta'$ has form $\theta_k'=\frac{\alpha_k+N_k-1}{\sum_l(\alpha_l+N-K)}$

$L_2$: The expectation has the same form as in the posterior predictive in problem 3: $\theta_k'=\frac{\inf_{S_K}\theta_k\prod_{k=1}^{K}\theta_k^{N_k+\alpha_k-1}d\theta}{B(\alpha_1+N_1,\cdots,\alpha_K+N_K)}=\frac{\alpha_k+N_k}{\sum_l\alpha_l+N}$

id: 82fd50678b984c07b5707323e5075a21
parent_id: c75275f606794c92a7c5cd40939865ab
created_time: 2023-03-26T03:26:51.962Z
updated_time: 2023-07-02T07:59:12.257Z
is_conflict: 0
latitude: 0.00000000
longitude: 0.00000000
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 1680266801839
user_created_time: 2023-03-26T03:26:51.962Z
user_updated_time: 2023-07-02T07:59:12.257Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
type_: 1