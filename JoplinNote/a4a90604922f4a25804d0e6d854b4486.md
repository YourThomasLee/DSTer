2.4.2 模型部署

总的来说，业界主流的**模型服务方法**有 4 种，分别是预存推荐结果或 Embedding 结果、预训练 Embedding+ 轻量级线上模型、PMML 模型以及 TensorFlow Serving。

* **预存推荐结果或 Embedding 结果**：最简单直接的模型服务方法就是在离线环境下生成对每个用户的推荐结果，然后将结果预存到以 Redis 为代表的线上数据库中。由于以下的优缺点，这种直接存储推荐结果的方式往往只适用于用户规模较小，或者一些冷启动、热门榜单等特殊的应用场景中。

![image.png](:/ce31ce3e815e41b4a133df1541fa52e2)

* **预训练 Embedding+ 轻量级线上模型**： 直接预存 Embedding 的方法让模型表达能力受限这个问题的产生，主要是因为我们仅仅采用了“相似度计算”这样非常简单的方式去得到最终的推荐分数。线上部分从 Redis 之类的模型数据库中拿到离线生成 Embedding 向量，然后跟其他特征的 Embedding 向量组合在一起，扔给一个标准的多层神经网络进行预估，这就是一个典型的“预训练 Embedding+ 轻量级线上模型”的服务方式。
* **利用 PMML 转换和部署模型**：PMML 的全称是“预测模型标记语言”(Predictive Model Markup Language, PMML)，它是一种通用的以 XML 的形式表示不同模型结构参数的标记语言。在模型上线的过程中，PMML 经常作为中间媒介连接离线训练平台和线上预测平台。以 Spark MLlib 模型的训练和上线过程为例，来和你详细解释一下，PMML 在整个机器学习模型训练及上线流程中扮演的角色![image.png](:/ec4a7afb5e2c49e0a522d816a1b09ab4)  
  例子使用了 JPMML 作为序列化和解析 PMML 文件的 library（库），JPMML 项目分为 Spark 和 Java Server 两部分。Spark 部分的 library 完成 Spark MLlib 模型的序列化，生成 PMML 文件，并且把它保存到线上服务器能够触达的数据库或文件系统中，而 Java Server 部分则完成 PMML 模型的解析，生成预估模型，完成了与业务逻辑的整合。让模型完全是 End2End（端到端）训练 +End2End 部署。与 JPMML 相似的开源项目还有 MLeap，同样采用了 PMML 作为模型转换和上线的媒介。[JPMML](https://github.com/jpmml)和[MLeap](https://github.com/combust/mleap)的项目地址。
* **TensorFlow Serving**：对于具有复杂结构的深度学习模型来说，PMML 语言的表示能力还是比较有限的，还不足以支持复杂的深度学习模型结构。  
  从整体工作流程来看，TensorFlow Serving 和 PMML 类工具的流程一致，它们都经历了模型存储、模型载入还原以及提供服务的过程。在具体细节上，TensorFlow 在离线把模型序列化，存储到文件系统，TensorFlow Serving 把模型文件载入到模型服务器，还原模型推断过程，对外以 HTTP 接口或 gRPC 接口的方式提供模型服务。  
  ​![image.png](:/ff3600bafaf24ac380938c171f19d95e)

  具体到咱们的 Sparrow Recsys 项目中，我们会在离线使用 TensorFlow 的 Keras 接口完成模型构建和训练，再利用 TensorFlow Serving 载入模型，用 Docker 作为服务容器，然后在 Jetty 推荐服务器中发出 HTTP 请求到 TensorFlow Serving，获得模型推断结果，最后推荐服务器利用这一结果完成推荐排序。docker官网教程：https://www.docker.com/get-started/。利用 Docker 命令拉取 TensorFlow Serving 的镜像:

  ```shell
  # 从docker仓库中下载tensorflow/serving镜像
  docker pull tensorflow/serving
  ```

  再从 TenSorflow 的官方 GitHub 地址下载 TensorFlow Serving 相关的测试模型文件：

  ```shell
  # 把tensorflow/serving的测试代码clone到本地
  git clone https://github.com/tensorflow/serving
  # 指定测试数据的地址
  TESTDATA="$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata"

  # 下载tensorflow
  docker pull tensorflow/tensorflow:latest  # Download latest stable image
  docker run -it -p 8888:8888 tensorflow/tensorflow:latest-jupyter  # Start Jupyter server 
  # 开启编码环境
  ```

  最后，我们在 Docker 中启动一个包含 TensorFlow Serving 的模型服务容器，并载入我们刚才下载的测试模型文件 half_plus_two：

  ```shell
  # 启动TensorFlow Serving容器，在8501端口运行模型服务API
  docker run -t --rm -p 8501:8501 \
      -v "$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two" \
      -e MODEL_NAME=half_plus_two \
      tensorflow/serving &
  ```

  在命令执行完成后，如果你在 Docker 的管理界面中看到了 TenSorflow Serving 容器，如下图所示，就证明 TensorFlow Serving 服务被你成功建立起来了。

  请求 TensorFlow Serving 获得预估结果：我们再来验证一下是否能够通过 HTTP 请求从 TensorFlow Serving API 中获得模型的预估结果。我们可以通过 curl 命令来发送 HTTP POST 请求到 TensorFlow Serving 的地址，或者利用 Postman 等软件来组装 POST 请求进行验证。

  ```shell
  # 请求模型服务API
  curl -d '{"instances": [1.0, 2.0, 5.0]}' \
      -X POST http://localhost:8501/v1/models/half_plus_two:predict
  ```

  如果你看到了下图这样的返回结果，就说明 TensorFlow Serving 服务已经成功建立起来了

  ```shell
  # 返回模型推断结果如下
  # Returns => { "predictions": [2.5, 3.0, 4.5] }
  ```

  TensorFlow Serving 的官方教程： https://www.tensorflow.org/tfx/serving/docker

![image.png](:/32d16e7eb3de41cf9d7b6edb2a81677d)


id: a4a90604922f4a25804d0e6d854b4486
parent_id: a73de12938e94140aa3929fc11fd38ae
created_time: 2023-03-25T10:09:45.379Z
updated_time: 2023-03-31T12:47:40.705Z
is_conflict: 0
latitude: 0.00000000
longitude: 0.00000000
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 1680266860583
user_created_time: 2023-03-25T10:09:45.379Z
user_updated_time: 2023-03-31T12:47:40.705Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
type_: 1