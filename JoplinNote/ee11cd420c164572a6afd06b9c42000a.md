2.2.2 模型训练

训练模式：

* point-wise [Embedding-based retrieval in Facebook search]: 把召回看做简单的二分类任务{-1, +1}，控制正负样本比例为1：2或1：3
* pair-wise [Sampling-bias-corrected neural model for large corpus item]：扩大正样本和负样本之间的距离。精简版的排序模型
* list-wise：一个用户、一个正样本和多个负样本。鼓励正样本分数尽可能的大，负样本分数尽可能小。softmax函数产出预测，使用交叉熵损失函数

正负样本选择：50%负样本是全体物品，50%使没通过排序的物品

* 正样本：用户点击的物品。正样本中存在少部分物品占据大部分点击的问题，导致正样本大部分是热门物品，解决的方法之一是过采样冷门物品，降采样热门物品。
* 负样本

  * 没有被召回的：从全体物品中抽样。负样本抽样概率公式打压热门程度（$\alpha^{0.75}$,$\alpha$为点击次数）
  * batch内负样本：所有样本都是正样本，正样本做其他人的负样本。存在热门样本过多次成为负样本，过度打压热门样本。修正方式为训练相似度计算为$cos(u, s_i) - \log p_i, 其中概率p_i与点击次数正相关$
  * 召回但是被粗排、精排淘汰的：

    * 被粗排淘汰的物品（比较困难）
    * 精排分数靠后的物品（非常困难）
  * 曝光但是未点击的：不适合用作召回负样本，只适合用作排序训练样本。根本原因在于它不能界定用户是否感兴趣的标志，不点击也可能有兴趣。

线上更新：增量更新可能有偏差，另外样本按时间顺序到来，容易损害模型的泛华能力（陷入局部最优然后跳不出来？）全量更新打乱一天的数据进行训练效果更好。

* 全量更新：基于已有的模型在新的数据上跑一个epoch更新所有参数，包括用户和物品的embedding、网络的参数等等。

* 增量更新：最近一小时的数据，只用来更新embedding，网络参数锁定


id: ee11cd420c164572a6afd06b9c42000a
parent_id: f91d99bae8504618b2112052e8e244dc
created_time: 2023-03-25T10:09:46.731Z
updated_time: 2023-03-31T12:47:38.777Z
is_conflict: 0
latitude: 0.00000000
longitude: 0.00000000
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 1680266858759
user_created_time: 2023-03-25T10:09:46.731Z
user_updated_time: 2023-03-31T12:47:38.777Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
type_: 1