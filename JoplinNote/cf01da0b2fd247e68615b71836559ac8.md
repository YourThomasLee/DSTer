An introduction to dialogue systems

# Introduction

pas

# Existing methods<sup>[\[1\]](#fn1)</sup>

you can check it

## Memory networks<sup>[\[2\]](#fn2)</sup>

The design ration of memory networks can be expressed as the storage of past experience or outside knowledge source that are useful in addressing problems.
Weston et al.(2015) proposed memory networks, a model endowed with a memory component. It has five modules:

- a memory module to stores the representations of memory facts;
- an "I" module maps the input memory facts into embedded representations;
- a "G" module decides the updates of the memory module;
- an "O" module generates the output conditioned on the input representation and memory representation;
- an "R" module organizes the final response based on the output of "O" module

Fig (5) represents the proposed end-to-end memory networks. It architecture consists of three stages: weight calculation, memory selection, and final prediction.
![41ab6d04cc123d55717a4dc7cc2a2851.png](:/878a1126ccf846d79f3ff78c9d3a4dcf)

- weight calculation: The model first converts the input memory set $\{x_i\}$ into memory representations $\{m_i\}$ using a representation model $A$. Then it maps the input query into its embedding space using
    another representation model $B$, obtaining an embedding vector $u$. The final weights are
    calculated as follows: $p_i = \text{Softmax}(u^Tm_i)$, where $p_i$ is the weight corresponding to each input memory $x_i$ conditioned on the query.
- memory selection: Before generating the final prediction, a selected memory vector is generated by first
    encoding the input memory $x_i$ into an embedded vector $c_i$ using another representation
    model $C$, then calculating the weighted sum over the$\{c_i\}$ using the weights calculated in
    the previous stage: $o = \sum_i p_i c_i$, where $o$ represents the selected memory vector. This vector cannot be found in memory representations. The soft memory selection facilitates differentiability in gradient computing, which makes the whole model end-to-end trainable
- final prediction: the prediction is obtained by mapping the sum vector of the selected memory $o$ and the embedded query $u$ into a probability vector $\hat a = \text{Softmax}(W(o +u ))$. Many dialogue-related works incorporate memory networks into their framework, especially for tasks involving an external knowledge base like task-oriented<sup>[\[3\]](#fn3)</sup> or knowledge-grounded dialogue systems.

## Attention Mechanism

pass

## Pointer Net and CopyNet

Traditional sequence-to-sequence models with an encoder–decoder structure map a source sentence to a target sentence. Generally, these models first map the source sentence into hidden state vectors with an encoder and
then predict the output sequence based on the hidden states. The sequence prediction is accomplished step-by-step, each step predicting one token using greedy search or beam search. The overall sequence-to-sequence model can be described by the following probability model: $P(C^P|P; \theta) = \prod_{i=1}^{m(P)}p(C_i|C_1, \cdots, C_{i-1}, P; \theta)$, where $(P,C_P)$ constitutes a training pair, $P=(P_1,\cdots,P_n)$ denotes the input sequence and $C_P= \{C_1,\cdots,C_{m(P)}\}$ denotes the ground target sequence. $\theta$ is a decoder model. Traditional attention based model calculates the weight of all hidden states then predict the next token $P(C_i|C_1,\cdots,C_{i-1}) = \sum_{j=1}^{T_x} \alpha_{ij}h_j$
![67fb5f2c75884cf9493d88ed85c92f35.png](:/e47f4f7e77d4446aae4298d0096947bd)

Pointer Net made a simple change to the attention-based sequence-to-sequence models: instead of predicting the token distribution based on the weighted sum of encoder hidden states di , it directly used the normalized weights $\alpha_i$ as predicted distribution: $P(C_i|C_1,\cdots,C_{i-1})=\alpha_i$, where $\alpha_i$ is a set of probability numbers ${\\alpha\_i^1, \\cdots, \\alpha\_i^j} which represents the probability distribution over the tokens of the input sequence. Obviously, the token prediction problem is now transformed into position prediction problem, where the model only needs to predict a position in the input sequence.

The encoder of CopyNet is the same as that of a traditional sequence-to-sequence model, whereas the decoder has some differences compared with a traditional attention-based decoder. When predicting the token at time step $t$, it combines the probabilistic models of generate-mode and copy-mode:$P(y_t|s_t, y_{t-1}, c_t, M) = P_g(y_t|s_t, y_{t-1}, c_t, M) + P_c(y_t|s_t, y_{t-1}, c_t, M)$, where $t$ is the time step. $s_t$ is the decoder hidden state and $y_t$ is the predicted token. $c_t$ and $M$ represent weighted sum of encoder hidden states and encoder hidden states respectively. $g, c$ are generated-mode and copy-mode respectively. Besides, though it still uses $y_{t−1}$ and weighted attention vector $c_t$ to update the decoder hidden state, $y_{t−1}$ is uniquely encoded with both its embedding and its location-specific hidden state; also, CopyNet combines attentive read and selective read to capture information from the encoder hidden states, where the selective read is the same method used in PointerNet. The CopyNet has a location-based mechanism that enables the model to be aware of some specific details in training data in a more subtle way.

* * *

1.  Jinjie Ni, Tom Young, et al., Recent advances in deep learning based dialogue systems: a systematic survey [↩︎](#fnref1)
    
2.  Sukhbaatar S, Szlam A, Weston J, Fergus R (2015) End-to-end memory networks. In: Cortes C, Lawrence ND, Lee DD, Sugiyama M, Garnett R (eds) Advances in Neural Information Processing Systems [↩︎](#fnref2)
    
3.  Kim S, Yang S, Kim G, Lee SW (2020b) Efficient dialogue state tracking by selectively overwriting mem-
    ory. In: Proceedings of the 58th annual meeting of the association for computational linguistics [↩︎](#fnref3)

id: cf01da0b2fd247e68615b71836559ac8
parent_id: 7787b4a132dd42f6bf0d2a9100dcb9d5
created_time: 2023-04-10T04:53:19.481Z
updated_time: 2023-04-18T04:18:54.470Z
is_conflict: 0
latitude: 31.23041600
longitude: 121.47370100
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 0
user_created_time: 2023-04-10T04:53:19.481Z
user_updated_time: 2023-04-18T04:18:54.470Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
type_: 1