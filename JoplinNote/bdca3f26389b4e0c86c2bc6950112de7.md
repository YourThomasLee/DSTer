2. Word Vectors GloVe, Evaluation and Training

Here we first introduces the GloVe model for training word vectors. Then it extends our discussion of word vectors by seeing how they can be evaluated intrinsically and extrinsically. After that, we discuss training model weights/parameters and word vectors for extrinsic tasks. 


## 2.1 Gloal Vectors for word Representation (GloVe)

We have two main classes of methods to find word embeddings. The first set are **count-based** and rely on matrix fatorization (e.g.LSA, HAL), whose results are  effectively **leverage global statistical** information and good at word similarities capture, but it does poorly on tasks such as word analogy, indicating a sub-optimal vector space structure. The other set of methods are **shallow window-based** (e.g. the skip-gram and the CBOW models), whose results demonstrate the capacity to capture complex linguistic patterns beyond word similarity, but fail to make use of **the global co-occurrence statistics**.  

In comparison, GloVe consists of a weighted least squares model that trains on global word-word co-ocurrence counts and thus make efficient use of statistics. 

## 2.2 Co-ocurrence matrix

Let $X$ denote the word-word co-ocurrence matrix, where $X_{ij}$ indicates the number of times word $j$ occur in the context of word $i$. Let $X_i = \sum_k X_{ik}$ be the number of times any word $k$ appears in the context of word $i$. Finally, let $P_{ij} = P(w_j|w_i)=\frac{X_ij}{X_i}$ be the probability of $j$ appearing in the context of word $i$.

## 2.3 Least square objective

Recall that for the skip-gram model, we use softmax to compute probability of word $j$ appears in the context of word $i$: $Q_{ij} = \frac{\exp (\vec {u}_j^T \vec{v}_i)}{\sum_{w=1}^W \exp (\vec {u}_w^T \vec{v}_i))}$; Training prodeeds in an on-line, stochastic fashion, but the implied global cross-entropy loss can be calculated as: $J = - \sum_{i\in corpus}\sum_{j \in context(i)} \log Q_{ij}$.  
    **As the same words $i$ and $j$ can appear multiple times in the corpus**, it is more efficient to first group together the same values for $i$ and $j$: $J = -\sum_{i = 1}^W\sum_{j = 1}^W X_{ij}\log Q_{ij}$. One significant drawback of the cross-entropy loss is that it requires the distribution $Q$ to be properly nomalized, which **involves the expansive summation over the entire vocabulary**. Instead, we use a least square objective in which the normalization factors in $P$ and $Q$ are discarded: $J=\sum_{i = 1}^W\sum_{j = 1}^W X_{i}(\hat P_{ij} -\hat Q_{ij})^2$. where $\hat P_{ij} = X_{ij}$ and  $\hat Q_{ij} = exp(\vec u_j^T \vec v_i)$ are the unnormalized distributions.  
    **This formulation introduces a new problem $-X_{ij}$ often takes on very large values and makes the optimization difficult.** An effective change is to minimize the squared error of the logarithms of $\hat P$ and $\hat Q$: $J=\sum_{i = 1}^W\sum_{j = 1}^W X_{i}(\log \hat P_{ij} -\log \hat Q_{ij})^2=\sum_{i = 1}^W\sum_{j = 1}^W X_{i}(\vec u_j^T \vec v_i-\log X_{ij})^2$. A main drawback to this model is that it weighs all co-occurrences equally, even those that happen rarely or never. As a result, we introduce a more general weighting function, which we are free to take to depend on the context word as well: $J=\sum_{i = 1}^W\sum_{j = 1}^W f(X_{i})(\vec u_j^T \vec v_i-\log X_{ij})^2$, where $f(x) = (x/x_{max})^{\alpha}, \; if\;x<x_{max}\;otherwise\;1$,  in which $x_{max}$ and $\alpha$ are hyper-parameter (in GloVe $x_{max} = 100, \alpha=3/4$).

## 2.4 Conclusion

The GloVe model efficiently leverages global statistical information by training only on the non-zero elements in a word-word co-occurence matrix, and produces a vector space with meaningful sub-structure. It consistently outperforms word2vec on the word analogy task, given the same corpus, vocabulary, window size, and training time. It achieves better results faster, and also obtains the best results irrespective of speed.


## 2.5 Evaluation of word vectors

In this section, we discuss how we can quantitatively evaluate the quality of word vectors.

* **Intrinsic evaluation** of word vectors is the evaluation of a set of word vectors generated by an embedding technique (such as Word2Vec or GloVe) on specific intermediate subtasks (such as analogy completion). An intrinsic evaluation should typically return to us a number that indicates the performance of those word vectors on the evaluation subtask. (1. evaluation on a specific, intermediate task; 2. fast to compute performance; 3. helps understand subsystem; 4. needs positive correlation with real task to determine usefulness)

  * Intrinsic evaluation example: Word vector analogies -  A popular choice for intrinsic evaluation of word vectors is its performance in completing word vector analogies.
  * Intrinsic evaluation tuning example: analogy evaluations
  * intrinsic evaluation example: correlation evaluation: Another simple way to evaluate the quality of word vectors is by asking humans to assess the similarity between two words on a fixed scale (say 0-10) and then comparing this with the cosine similarity between the corresponding word vectors
* **Extinsic evaluation** of word vectors is the evaluation of a set of word vectors generated by an embedding technique on the real task at hand. (1. Is the evaluation on a real task; 2. Can be slow to compute performance; 3. unclear if subsystem is the problem, or internal interactions; 4. If replacing subsystem improves performance, the change is likely good.)


id: bdca3f26389b4e0c86c2bc6950112de7
parent_id: 51a1435e58cb4207b3693dfbea6589d8
created_time: 2023-03-26T03:26:51.966Z
updated_time: 2023-03-31T12:46:42.257Z
is_conflict: 0
latitude: 0.00000000
longitude: 0.00000000
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 1680266802248
user_created_time: 2023-03-26T03:26:51.966Z
user_updated_time: 2023-03-31T12:46:42.257Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
type_: 1