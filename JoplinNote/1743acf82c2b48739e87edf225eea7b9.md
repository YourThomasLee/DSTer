2.3 排序技术

“排序层”则是利用排序模型对初筛的候选集进行精排序。

不同结构的深度学习模型在排序层的应用。排序层（也称精排层）是影响推荐效果的重中之重，也是深度学习模型大展拳脚的领域。深度学习模型的灵活性高，表达能力强的特点，这让它非常适合于大数据量下的精确排序。深度学习排序模型毫无疑问是业界和学界都在不断加大投入，快速迭代的部分。

增强学习在模型更新、工程模型一体化方向上的应用。增强学习可以说是与深度学习密切相关的另一机器学习领域，它在推荐系统中的应用，让推荐系统可以在实时性层面更上一层楼。


1. Linear Regression： 主要学习特征与标记的线性关系是机器学习的起点。简单直白，各特征的影响清清楚楚，欠缺之处在于表达能力有限，不足以表征数据内在的复杂规律和分布，容易欠拟合，如果有很强的专家知识提炼出高信息量的特征，面对各种后浪依然可以一战。

2. Mixed Logistic Regression： 通过两阶段分类器进行非线性拟合，先对样本聚类分组（softmax），然后在组内利用线性模型预测（logistic regression），最后加权求和，其中第一步聚类是软分类，算出样本对不同类别隶属度作为后面求和时的权重。整个过程也可以看做综合不同专家意见最后确定结论。

3. Descion Tree：从根到叶子节点的路径可以看做带有继承关系的层级结构，最终的落脚点在叶子节点上。

应用场景1：特征组合。Facebook利用GBDT来进行特征组合，在NN兴起之前是特征组合界最靓的仔.
应用场景2：高效检索。阿里在TMD论文中提出利用树的分层结构检索全量候选，大幅减少计算量从而提升检索效率，召回环节运用复杂模型的性能问题得到解决。

4. Factorization Machine： 二阶特征组合自动化。针对二级组合的参数提出了假设：通过两个特征隐向量的点积获得参数(思路看起来是不是有点眼熟)，解决了样本数据稀疏时的训练问题，提升了模型的泛化能力，其中隐向量就是NN中embedding后的向量表示，相比协同过滤中隐向量包含的信息，FM可以加入各种边信息含义更丰富。

5. Field-aware Factorization Machines： 在FM的基础上引入领域的概念，与不同领域的特征做组合时用不同的向量表达(f个领域则每个特征有f-1个向量表示)，对真相刻画的更细致，缺点在于参数太多(f*n*k)，在高性能要求下大规模上场发挥的空间有限。

在FM的基础上引入领域的概念，与不同领域的特征做组合时用不同的向量表达(f个领域则每个特征有f-1个向量表示)，对真相刻画的更细致，缺点在于参数太多(f*n*k)，在高性能要求下大规模上场发挥的空间有限。

6. NN

通过MLP和激活函数隐式提取高阶特征组合，从此机器学习迈入深度纪元。![image.png](:/45c63dc938ad46399334fc045932db84)![image.png](:/77ae108c486c464fbefb7b1986a517df)

神经网络历史上一度被遗弃到现在如火如荼，其中很关键的一个转折是反向传播的提出：运用链式法则求导来迭代参数，在数据和算力的推动下NN重返舞台，针对NN稍微展开聊几个有意思的点。

1）双塔结构

用户塔使用用户侧特征经过多次组合获得向量表示，广告塔获得广告表示，二者在学习过程中不交互，确定表示后通过点积/余弦相似度计算相关度，双塔的重点在于学习向量表示，是召回阶段的主力模型。

2）Embedding

广告召回排序涉及很多类别特征，针对离散化数据一般使用独热编码，考虑到训练成本和过拟合风险进入网络前会映射为低维稠密向量，就是embedding，这里它的主要作用是降维，其优秀之处在于获得的向量不仅维数减少而且保留了深层语义信息，可以进行表征、比较和运算。

3）Attention：不一视同仁根据实际情况区分主次，从而进行更细致的刻画。

attention的关键点在于如何确定重要度，谷歌在论文中提到Query、Key和Value，计算Query和Key的相关度作为作为权重，权重表示信息的重要程度，Value对应信息，权重越大对应Value给与更高的重视。

attention思想自提出以来得到广泛认同和使用，尤其在处理用户历史行为方面，机器学习的主要目标之一就是对数据进行辨识区分，attention很好的体现了这一点。

4）Pooling：pooling体现的是提取和压缩，CNN中通过pooling将多个向量压缩成一个向量，在处理历史行为序列时不同用户长度不同，一般也会做压缩提炼为确定的长度，pooling的方法有很多如max/sum/average等，当然还有上面提到的attention。

7. wide&deep：整体模型分为两部分，Wide部分是LR，负责显式提取低阶特征，强调记忆，特征组合需要人工构建，Deep部分一般使用前馈神经网络，负责隐式提取高阶特征，强调泛化，两个部分通过并行方式组合，低高阶特征共同作用于输出。

8. deepFM：模型整体结构与W&D一致，区别在于Wide部分由LR替换为FM，相比LR它能自动提取二阶特征组合，规避了依赖人工的弊端，且两部分共用embedding提升了效率，模型能抗能打是居家旅行必备之选。

9. Deep Cross Network：与deepFM一样也是在Wide部分动手脚，用Cross Network（$x_l = x_0x_{l-1}^Tw_l + b_l + x_{l - 1}$）显式提取特征，每层都对特征交叉组合，随着层数增加组合的阶数也持续增加，网络最后一层涵盖了从一阶到N+1阶的特征，根据实际情况来确定层数，不过既然有专门负责提取高阶的DNN工具在前，层数不宜太高。

![image.png](:/b48a18cd4a0a4c6ca5b18554f0eb59f9)



id: 1743acf82c2b48739e87edf225eea7b9
parent_id: 8af0105890f3497ab59d5e8eddee68d0
created_time: 2023-03-25T10:09:45.179Z
updated_time: 2023-03-31T12:47:40.474Z
is_conflict: 0
latitude: 0.00000000
longitude: 0.00000000
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 1680266859840
user_created_time: 2023-03-25T10:09:45.179Z
user_updated_time: 2023-03-31T12:47:40.474Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
type_: 1